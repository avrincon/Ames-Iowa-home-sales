{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f96f2a",
   "metadata": {},
   "source": [
    "# Scenario 2\n",
    "\n",
    "## Aim\n",
    "\n",
    "- Use Lasso penalized regression to determine the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f915d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af21f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent \"None\" from being read as NA\n",
    "df = pd.read_csv(\n",
    "    \"../data/ames_housing_clean_1.csv\",\n",
    "    keep_default_na=False,  \n",
    "    na_values=[\"\", \" \"],\n",
    "    dtype={'MSSubClass': 'str'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a59f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target variable (SalePrice) statistics:\")\n",
    "print(df['SalePrice'].describe().round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(['PID', 'SalePrice'], axis=1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84476a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there should be no missing data\n",
    "print(f\"Is there any missing data? {X.isna().any().any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f882a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "# For numeric features: StandardScaler\n",
    "# For categorical features: OneHotEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        # don't need to drop for a lasso model\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ba491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nProcessed feature matrix shape: {X_train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "# numeric_feature_names = numeric_features\n",
    "\n",
    "# Get categorical feature names after one-hot encoding\n",
    "cat_encoder = preprocessor.named_transformers_['cat']\n",
    "categorical_feature_names = cat_encoder.get_feature_names_out(categorical_features).tolist()\n",
    "\n",
    "all_feature_names = numeric_features + categorical_feature_names\n",
    "print(f\"Total features after preprocessing: {len(all_feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68636676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Lasso with cross-validation to find optimal alpha\n",
    "lasso_cv = LassoCV(\n",
    "    alphas=np.logspace(-4, 2, 50),  # Range of alpha values to try\n",
    "    cv=5,                         \n",
    "    random_state=42,\n",
    "    max_iter=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv.fit(X_train_processed, y_train)\n",
    "\n",
    "print(f\"Optimal alpha: {lasso_cv.alpha_:.6f}\")\n",
    "print(f\"Cross-validation score (R²): {lasso_cv.score(X_train_processed, y_train):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Lasso model with optimal alpha\n",
    "lasso_final = Lasso(alpha=lasso_cv.alpha_, random_state=42, max_iter=3000)\n",
    "lasso_final.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = lasso_final.predict(X_train_processed)\n",
    "y_test_pred = lasso_final.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919878fe",
   "metadata": {},
   "source": [
    "- **Do different scenario with R2 as metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n=== MODEL PERFORMANCE ===\")\n",
    "print(f\"Training R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "print(f\"Training RMSE: {train_rmse:,.2f}\")\n",
    "print(f\"Test RMSE: {test_rmse:,.2f}\")\n",
    "print(f\"Training MAE: {train_mae:,.2f}\")\n",
    "print(f\"Test MAE: {test_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cef6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "coefficients = lasso_final.coef_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'coefficient': coefficients,\n",
    "    'abs_coefficient': np.abs(coefficients)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "\n",
    "# # Get feature names in the correct order from the fitted preprocessor\n",
    "# feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# # Feature importance analysis with correct alignment\n",
    "# coefficients = lasso_final.coef_\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'feature': feature_names,\n",
    "#     'coefficient': coefficients,\n",
    "#     'abs_coefficient': np.abs(coefficients)\n",
    "# }).sort_values('abs_coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count non-zero coefficients\n",
    "non_zero_features = np.sum(coefficients != 0)\n",
    "print(f\"\\nFeature Selection Results:\")\n",
    "print(f\"Non-zero coefficients: {non_zero_features} out of {len(coefficients)}\")\n",
    "print(f\"Features eliminated: {len(coefficients) - non_zero_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef874b",
   "metadata": {},
   "source": [
    "- Remove Condtion2 column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 most important features\n",
    "feature_importance.loc[:20, [\"feature\", \"coefficient\"]].head(20).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 13))\n",
    "\n",
    "# Plot 1: Actual vs Predicted (Test Set)\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Sale Price')\n",
    "plt.ylabel('Predicted Sale Price')\n",
    "plt.title(f'Actual vs Predicted (Test Set)\\nR² = {test_r2:.3f}')\n",
    "\n",
    "# Plot 2: Residuals\n",
    "plt.subplot(2, 3, 2)\n",
    "residuals = y_test - y_test_pred\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Sale Price')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Test Set)')\n",
    "\n",
    "# Plot 3: Feature Importance (Top 15)\n",
    "plt.subplot(2, 3, 3)\n",
    "top_15 = feature_importance.head(15)\n",
    "plt.barh(range(len(top_15)), top_15['abs_coefficient'])\n",
    "plt.yticks(range(len(top_15)), [f[:15] + '...' if len(f) > 15 else f for f in top_15['feature']])\n",
    "plt.xlabel('Absolute Coefficient Value')\n",
    "plt.title('Top 15 Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Plot 4: Cross-validation path\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.semilogx(lasso_cv.alphas_, lasso_cv.mse_path_.mean(axis=1))\n",
    "plt.axvline(lasso_cv.alpha_, color='r', linestyle='--', label=f'Optimal α = {lasso_cv.alpha_:.6f}')\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Lasso CV Path')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 5: Prediction distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(y_test, alpha=0.7, label='Actual', bins=30)\n",
    "plt.hist(y_test_pred, alpha=0.7, label='Predicted', bins=30)\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Actual vs Predicted Prices')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 6: Feature coefficient distribution\n",
    "plt.subplot(2, 3, 6)\n",
    "non_zero_coefs = coefficients[coefficients != 0]\n",
    "plt.hist(non_zero_coefs, bins=30)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Distribution of Non-Zero Coefficients\\n({len(non_zero_coefs)} features)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be74fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(f\"\\n=== MODEL SUMMARY ===\")\n",
    "print(f\"Model Type: Lasso Linear Regression\")\n",
    "print(f\"Regularization Parameter (α): {lasso_cv.alpha_:.6f}\")\n",
    "print(f\"Total Features: {len(all_feature_names)}\")\n",
    "print(f\"Selected Features: {non_zero_features}\")\n",
    "print(f\"Feature Reduction: {((len(all_feature_names) - non_zero_features) / len(all_feature_names) * 100):.1f}%\")\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"  - R² Score: {test_r2:.4f}\")\n",
    "print(f\"  - RMSE: {test_rmse:,.2f}\")\n",
    "print(f\"  - MAE: {test_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d43a74",
   "metadata": {},
   "source": [
    "## Explore top features\n",
    "\n",
    "- Lasso regression model determined the following 11 features (when categorical featues are re-combined) to be the most important:\n",
    "`[\"Condition2\", \"RoofMatl\", \"Neighborhood\", \"GrLivArea\", \"PoolQC\", \"ExterQual\",\n",
    "\"BsmtQual\", \"BsmtExposure\", \"KitchenQual\", \"SaleCondition\", \"Exterior1st\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e33f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = [\"Condition2\", \"RoofMatl\", \"Neighborhood\", \"PoolQC\", \"ExterQual\",\n",
    "\"BsmtQual\", \"BsmtExposure\", \"KitchenQual\", \"SaleCondition\", \"Exterior1st\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_importance = feature_importance[feature_importance['feature'].isin(numeric_features) & \n",
    "                                        feature_importance['abs_coefficient'] > 0]\n",
    "\n",
    "\n",
    "print(f\"Numeric features selected: {len(numeric_importance)}/{len(numeric_features)}\")\n",
    "top_15_numeric_list = numeric_importance.feature.head(15)\n",
    "print(f\"\\n15 Most Important Numeric Features:\")\n",
    "numeric_importance.head(15).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd38f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_importance = feature_importance[~feature_importance['feature'].isin(numeric_features) & \n",
    "                                        feature_importance['abs_coefficient'] > 0]\n",
    "\n",
    "print(f\"Categorical features selected: {len(categorical_importance)}/{len(categorical_feature_names)}\")\n",
    "\n",
    "print(f\"\\n20 Most Important Categorical Features:\")\n",
    "categorical_importance.head(20).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a60deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature_list = categorical_importance['feature'].str.split('_').str[0].unique().tolist()\n",
    "top_15_cat_feature_list = cat_feature_list[:15]\n",
    "top_15_cat_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_violin(df, feature, target='SalePrice', figsize=(12, 8), \n",
    "                          show_stats=False, rotation_threshold=5, point_alpha=0.3, \n",
    "                          point_size=10):\n",
    "    \"\"\"\n",
    "    Create a violin plot showing the distribution of target variable by categorical feature\n",
    "    with median markers (squares) and individual data points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input dataframe containing the data\n",
    "    feature : str\n",
    "        Name of the categorical feature column to plot\n",
    "    target : str, default 'SalePrice'\n",
    "        Name of the target variable column\n",
    "    figsize : tuple, default (12, 8)\n",
    "        Figure size as (width, height)\n",
    "    show_stats : bool, default True\n",
    "        Whether to print summary statistics\n",
    "    rotation_threshold : int, default 5\n",
    "        Rotate x-axis labels if number of categories exceeds this threshold\n",
    "    point_alpha : float, default 0.3\n",
    "        Transparency of individual data points (0-1)\n",
    "    point_size : int, default 10\n",
    "        Size of individual data points\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if feature not in df.columns:\n",
    "        raise ValueError(f\"Feature '{feature}' not found in dataframe columns\")\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Target '{target}' not found in dataframe columns\")\n",
    "    \n",
    "    # Remove missing values\n",
    "    df_clean = df[[feature, target]].dropna()\n",
    "    \n",
    "    if df_clean.empty:\n",
    "        raise ValueError(f\"No valid data found for {feature} and {target}\")\n",
    "    \n",
    "    # Calculate mean prices by category and sort in descending order\n",
    "    mean_prices = df_clean.groupby(feature)[target].mean().sort_values(ascending=False)\n",
    "    order = mean_prices.index.tolist()\n",
    "    \n",
    "    # Create the violin plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create violin plot data and calculate sample sizes\n",
    "    violin_data = [df_clean[df_clean[feature] == cat][target].values for cat in order]\n",
    "    sample_sizes = [len(data) for data in violin_data]\n",
    "    \n",
    "    # Create labels with sample sizes\n",
    "    labels_with_n = [f\"{cat}\\n(n={n})\" for cat, n in zip(order, sample_sizes)]\n",
    "    \n",
    "    # Create violin plot\n",
    "    parts = plt.violinplot(violin_data, positions=range(1, len(order) + 1), \n",
    "                          showmeans=False, showmedians=False, showextrema=False)\n",
    "    \n",
    "    # Color the violin plots\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(order)))\n",
    "    for i, (pc, color) in enumerate(zip(parts['bodies'], colors)):\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Add individual data points and median squares\n",
    "    for i, (cat, data) in enumerate(zip(order, violin_data)):\n",
    "        x_pos = i + 1\n",
    "        \n",
    "        # Add jittered points for all data\n",
    "        x_jitter = np.random.normal(x_pos, 0.04, len(data))\n",
    "        plt.scatter(x_jitter, data, alpha=point_alpha, s=point_size, \n",
    "                   color='black', edgecolors='white', linewidth=0.5)\n",
    "        \n",
    "        # Add median as a square\n",
    "        median_val = np.median(data)\n",
    "        plt.scatter(x_pos, median_val, marker='s', s=80, color='red', \n",
    "                   edgecolors='white', linewidth=2, zorder=10, label='Median' if i == 0 else \"\")\n",
    "    \n",
    "    # Set labels and ticks\n",
    "    plt.xticks(range(1, len(order) + 1), labels_with_n)\n",
    "    plt.xlabel(feature, fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(f'{target} ($)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Rotate x-labels if there are many categories\n",
    "    if len(order) > rotation_threshold:\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    # Format y-axis to show prices in thousands\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(f'{target} Distribution by {feature}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add legend for median squares\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print summary statistics if requested\n",
    "    if show_stats:\n",
    "        print(f\"\\nStatistics for {target} by {feature} (sorted by mean, highest to lowest):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Category':<20} {'Mean':<12} {'Median':<12} {'Std':<12} {'Count':<8}\")\n",
    "        print(\"-\" * 80)\n",
    "        for category in order:\n",
    "            cat_data = df_clean[df_clean[feature] == category][target]\n",
    "            mean_price = cat_data.mean()\n",
    "            median_price = cat_data.median()\n",
    "            std_price = cat_data.std()\n",
    "            count = len(cat_data)\n",
    "            print(f\"{category:<20} ${mean_price:<11,.0f} ${median_price:<11,.0f} ${std_price:<11,.0f} {count:<8}\")\n",
    "    \n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in top_15_cat_feature_list:\n",
    "    plot_categorical_violin(df, feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff43248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_scatter(df, feature, target='SalePrice', figsize=(10, 6), \n",
    "                        show_trendline=False, log_scale=False, point_alpha=0.6, \n",
    "                        point_size=50, show_stats=False):\n",
    "    \"\"\"\n",
    "    Create a scatter plot showing the relationship between a numeric feature and target variable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input dataframe containing the data\n",
    "    feature : str\n",
    "        Name of the numeric feature column to plot\n",
    "    target : str, default 'SalePrice'\n",
    "        Name of the target variable column\n",
    "    figsize : tuple, default (10, 6)\n",
    "        Figure size as (width, height)\n",
    "    show_trendline : bool, default True\n",
    "        Whether to show a linear trend line\n",
    "    log_scale : bool, default False\n",
    "        Whether to use log scale for both axes\n",
    "    point_alpha : float, default 0.6\n",
    "        Transparency of data points (0-1)\n",
    "    point_size : int, default 50\n",
    "        Size of data points\n",
    "    show_stats : bool, default True\n",
    "        Whether to print correlation statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if feature not in df.columns:\n",
    "        raise ValueError(f\"Feature '{feature}' not found in dataframe columns\")\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Target '{target}' not found in dataframe columns\")\n",
    "    \n",
    "    # Remove missing values\n",
    "    df_clean = df[[feature, target]].dropna()\n",
    "    \n",
    "    if df_clean.empty:\n",
    "        raise ValueError(f\"No valid data found for {feature} and {target}\")\n",
    "    \n",
    "    # Check if feature is numeric\n",
    "    if not pd.api.types.is_numeric_dtype(df_clean[feature]):\n",
    "        raise ValueError(f\"Feature '{feature}' is not numeric\")\n",
    "    \n",
    "    # For log scale, ensure positive values only\n",
    "    if log_scale:\n",
    "        df_clean = df_clean[(df_clean[feature] > 0) & (df_clean[target] > 0)]\n",
    "        if df_clean.empty:\n",
    "            raise ValueError(f\"No positive values found for log scale plotting\")\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.scatter(df_clean[feature], df_clean[target], alpha=point_alpha, s=point_size, \n",
    "               color='steelblue', edgecolors='white', linewidth=0.5)\n",
    "    \n",
    "    # Add trend line if requested\n",
    "    if show_trendline:\n",
    "        # For log scale, we'll fit on log-transformed data\n",
    "        if log_scale:\n",
    "            # Fit on log-transformed data\n",
    "            log_x = np.log(df_clean[feature])\n",
    "            log_y = np.log(df_clean[target])\n",
    "            z = np.polyfit(log_x, log_y, 1)\n",
    "            \n",
    "            # Create trend line points\n",
    "            x_range = np.linspace(df_clean[feature].min(), df_clean[feature].max(), 100)\n",
    "            log_x_range = np.log(x_range)\n",
    "            log_y_trend = z[0] * log_x_range + z[1]\n",
    "            y_trend = np.exp(log_y_trend)\n",
    "            \n",
    "            plt.plot(x_range, y_trend, \"r--\", alpha=0.8, linewidth=2, label='Trend line')\n",
    "        else:\n",
    "            # Regular linear trend line\n",
    "            z = np.polyfit(df_clean[feature], df_clean[target], 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_range = np.linspace(df_clean[feature].min(), df_clean[feature].max(), 100)\n",
    "            plt.plot(x_range, p(x_range), \"r--\", alpha=0.8, linewidth=2, label='Trend line')\n",
    "    \n",
    "    # Set log scale if requested\n",
    "    if log_scale:\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        scale_text = \" (Log Scale)\"\n",
    "    else:\n",
    "        scale_text = \"\"\n",
    "    \n",
    "    # Set labels\n",
    "    plt.xlabel(f'{feature}', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(f'{target} ($)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Format y-axis (only for non-log scale)\n",
    "    if not log_scale:\n",
    "        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "    else:\n",
    "        # For log scale, use scientific notation or simplified format\n",
    "        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(f'{feature} vs {target}{scale_text}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend if trend line is shown\n",
    "    if show_trendline:\n",
    "        plt.legend()\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate and print statistics if requested\n",
    "    if show_stats:\n",
    "        correlation = df_clean[feature].corr(df_clean[target])\n",
    "        n_points = len(df_clean)\n",
    "        \n",
    "        print(f\"\\nStatistics for {feature} vs {target}:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Correlation coefficient: {correlation:.3f}\")\n",
    "        print(f\"Number of data points: {n_points:,}\")\n",
    "        print(f\"Feature range: {df_clean[feature].min():,.0f} to {df_clean[feature].max():,.0f}\")\n",
    "        print(f\"Target range: ${df_clean[target].min():,.0f} to ${df_clean[target].max():,.0f}\")\n",
    "        \n",
    "        if log_scale:\n",
    "            # Additional stats for log scale\n",
    "            log_correlation = np.log(df_clean[feature]).corr(np.log(df_clean[target]))\n",
    "            print(f\"Log-log correlation: {log_correlation:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd6739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in top_15_numeric_list:\n",
    "    plot_numeric_scatter(df, feature, figsize=(8, 5))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
