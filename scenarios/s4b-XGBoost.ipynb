{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8b7158",
   "metadata": {},
   "source": [
    "# Scenario 7 - XGBoost native\n",
    "\n",
    "* Use XGBoost Regressor model to predict log SalePrice\n",
    "* Allow native handling of missing values and categorical data\n",
    "* Selected features based on results of SequentialFeatureSelector and Lasso models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcddd62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ccb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\n",
    "    \"../data/Ames_Housing_Price_Data.csv\",\n",
    "    keep_default_na=False,  \n",
    "    na_values=[\"\", \" \"],\n",
    "    dtype={'MSSubClass': 'str'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79cb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LogSalePrice target variable\n",
    "df['LogSalePrice'] = np.log(df['SalePrice'])\n",
    "\n",
    "# Define selected features\n",
    "selected_features = [\n",
    "    'BldgType', 'BsmtExposure', 'BsmtFinSF1', 'BsmtQual', 'Condition2', \n",
    "    'ExterCond', 'ExterQual', 'Exterior1st', 'Functional', 'GarageArea', \n",
    "    'GarageCond', 'GrLivArea', 'KitchenQual', 'MSSubClass', 'MSZoning', \n",
    "    'MasVnrArea', 'Neighborhood', 'OverallCond', 'OverallQual', 'RoofMatl', \n",
    "    'SaleCondition', 'TotalBsmtSF', 'YearBuilt', 'YearRemodAdd'\n",
    "]\n",
    "\n",
    "# Select features and target\n",
    "X = df[selected_features]\n",
    "y = df['LogSalePrice']\n",
    "\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and print the total number of missing values\n",
    "missing_y = y.isna().sum().sum()\n",
    "total_missing_values = X.isna().sum().sum()\n",
    "print(f\"Total number of missing y values: {missing_y}\")\n",
    "print(f\"Total number of missing X values: {total_missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5906e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa2587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing_values = X.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "missing_values = missing_values.sort_values(ascending=False)\n",
    "missing_to_replace = [c for c in categorical_features]\n",
    "# replace missing values in categorical columns with \"None\"\n",
    "for val in missing_values.index:\n",
    "    if val in missing_to_replace:\n",
    "        X[val] = X[val].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing_values = X.isna().sum().sum()\n",
    "print(f\"Total number of missing X values after 'None' replaced: {total_missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = X.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "missing_values = missing_values.sort_values(ascending=False)\n",
    "print(f\"True missing values:\\n{missing_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert selected features to 'object' type to treat them as categorical\n",
    "for col in categorical_features:\n",
    "    X[col] = X[col].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ba9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed feature matrix shape: {X_train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d1578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearch\n",
    "print(f\"\\n=== HYPERPARAMETER TUNING ===\")\n",
    "print(\"Starting grid search for optimal hyperparameters...\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'subsample': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "xgb_grid = xgb.XGBRegressor(random_state=42, verbosity=0)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_grid, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='r2', \n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e3d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_test_pred = best_xgb.predict(X_test_processed)\n",
    "y_train_pred = best_xgb.predict(X_train_processed)\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# Final model summary\n",
    "print(f\"\\n=== XGBoost MODEL PERFORMANCE ===\")\n",
    "print(f\"Training R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Training MAE: {train_mae:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_names = (numeric_features + \n",
    "                preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist())\n",
    "\n",
    "# importance_scores = best_xgb.feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': best_xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n=== TOP 15 FEATURE IMPORTANCES ===\")\n",
    "feature_importance.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dcca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "cv_scores = cross_val_score(best_xgb, X_train_processed, y_train, cv=5, scoring='r2')\n",
    "print(f\"\\n=== CROSS-VALIDATION RESULTS ===\")\n",
    "print(f\"CV R² scores: {cv_scores}\")\n",
    "print(f\"Mean CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize=(20, 30))\n",
    "\n",
    "# Plot 1: Actual vs Predicted (Test Set)\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6, color='steelblue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Log(SalePrice)')\n",
    "plt.ylabel('Predicted Log(SalePrice)')\n",
    "plt.title(f'Actual vs Predicted (Test Set)\\nR² = {test_r2:.3f}')\n",
    "\n",
    "# Plot 2: Residuals\n",
    "plt.subplot(3, 2, 2)\n",
    "residuals = y_test - y_test_pred\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.6, color='steelblue')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Log(SalePrice)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Test Set)')\n",
    "\n",
    "# Plot 3: Feature Importance (Top 15)\n",
    "plt.subplot(3, 2, 3)\n",
    "top_15 = feature_importance.head(15)\n",
    "plt.barh(range(len(top_15)), top_15['importance'])\n",
    "plt.yticks(range(len(top_15)), [f[:20] + '...' if len(f) > 20 else f for f in top_15['feature']])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Top 15 Feature Importance (XGBoost)')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Plot 4: Training vs Test Performance\n",
    "plt.subplot(3, 2, 4)\n",
    "metrics = ['R²', 'RMSE', 'MAE']\n",
    "train_scores = [train_r2, train_rmse, train_mae]\n",
    "test_scores = [test_r2, test_rmse, test_mae]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, train_scores, width, label='Training', alpha=0.8)\n",
    "plt.bar(x + width/2, test_scores, width, label='Test', alpha=0.8)\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Training vs Test Performance')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "\n",
    "# Plot 5: Prediction distribution\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.hist(y_test, alpha=0.7, label='Actual', bins=30, color='steelblue')\n",
    "plt.hist(y_test_pred, alpha=0.7, label='Predicted', bins=30, color='orange')\n",
    "plt.xlabel('Log(SalePrice)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Actual vs Predicted')\n",
    "plt.legend()\n",
    "\n",
    "# # Plot 7: Feature importance by category\n",
    "# plt.subplot(3, 2, 6)\n",
    "# # Separate numeric and categorical feature importance\n",
    "# numeric_importance = feature_importance[feature_importance['feature'].isin(numeric_features)]\n",
    "# categorical_importance = feature_importance[~feature_importance['feature'].isin(numeric_features)]\n",
    "\n",
    "# categories = ['Numeric', 'Categorical']\n",
    "# importance_sums = [numeric_importance['importance'].sum(), categorical_importance['importance'].sum()]\n",
    "# plt.pie(importance_sums, labels=categories, autopct='%1.1f%%', startangle=90)\n",
    "# plt.title('Feature Importance by Type')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b7375",
   "metadata": {},
   "source": [
    "# Top Features vs Log SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "top_features = feature_importance.head(15).copy()\n",
    "# top_features = feature_importance.copy()\n",
    "# Extract base feature names by removing text after underscore\n",
    "top_features['feature'] = top_features['feature'].str.replace(r'_.*', '', regex=True)\n",
    "# print(top_features)\n",
    "# Group by the modified feature names and sum the importance values\n",
    "top_features = top_features.groupby('feature', as_index=False)['importance'].sum()\n",
    "top_features = top_features.sort_values('importance', ascending=False)\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_relationship(df, feature, target='LogSalePrice', figsize=(10, 6)):\n",
    "    \"\"\"Plot relationship between numeric feature and target\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Remove missing values for this feature\n",
    "    mask = df[feature].notna() & df[target].notna()\n",
    "    x_data = df.loc[mask, feature]\n",
    "    y_data = df.loc[mask, target]\n",
    "    \n",
    "    # Scatter plot with trend line\n",
    "    ax1.scatter(x_data, y_data, alpha=0.6, color='steelblue', s=20)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(x_data, y_data, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "    ax1.plot(x_range, p(x_range), \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = x_data.corr(y_data)\n",
    "    \n",
    "    ax1.set_xlabel(feature, fontweight='bold')\n",
    "    ax1.set_ylabel('LogSalePrice', fontweight='bold')\n",
    "    ax1.set_title(f'{feature} vs LogSalePrice\\nCorrelation: {correlation:.3f}', fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution plot\n",
    "    ax2.hist(x_data, bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    ax2.set_xlabel(feature, fontweight='bold')\n",
    "    ax2.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax2.set_title(f'Distribution of {feature}', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{feature} Statistics:\")\n",
    "    print(f\"  Correlation with LogSalePrice: {correlation:.4f}\")\n",
    "    print(f\"  Feature range: {x_data.min():.1f} to {x_data.max():.1f}\")\n",
    "    print(f\"  Feature mean: {x_data.mean():.1f}\")\n",
    "    print(f\"  Sample size: {len(x_data):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddb3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_relationship(df, \"OverallQual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_relationship(df, feature, target='LogSalePrice', figsize=(12, 8)):\n",
    "    \"\"\"Plot relationship between categorical feature and target\"\"\"\n",
    "    # Remove missing values\n",
    "    mask = df[feature].notna() & df[target].notna()\n",
    "    df_clean = df.loc[mask, [feature, target]].copy()\n",
    "    \n",
    "    if df_clean.empty:\n",
    "        print(f\"No valid data for {feature}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate statistics by category\n",
    "    stats_by_category = df_clean.groupby(feature)[target].agg([\n",
    "        'count', 'mean', 'median', 'std'\n",
    "    ]).round(3)\n",
    "    \n",
    "    # Sort categories by mean LogSalePrice\n",
    "    category_order = stats_by_category.sort_values('mean', ascending=False).index\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # Box plot\n",
    "    df_clean[feature] = pd.Categorical(df_clean[feature], categories=category_order, ordered=True)\n",
    "    sns.boxplot(data=df_clean, x=feature, y=target, ax=ax1)\n",
    "    ax1.set_title(f'{feature} vs {target} (Box Plot)', fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Violin plot\n",
    "    sns.violinplot(data=df_clean, x=feature, y=target, ax=ax2)\n",
    "    ax2.set_title(f'{feature} vs {target} (Violin Plot)', fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mean values bar plot\n",
    "    means = stats_by_category.loc[category_order, 'mean']\n",
    "    colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(means)))\n",
    "    bars = ax3.bar(range(len(means)), means.values, color=colors)\n",
    "    ax3.set_xlabel(feature, fontweight='bold')\n",
    "    ax3.set_ylabel(f'Mean {target}', fontweight='bold')\n",
    "    ax3.set_title(f'Mean {target} by {feature}', fontweight='bold')\n",
    "    ax3.set_xticks(range(len(means)))\n",
    "    ax3.set_xticklabels(means.index, rotation=45)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, means.values):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Sample size plot\n",
    "    counts = stats_by_category.loc[category_order, 'count']\n",
    "    ax4.bar(range(len(counts)), counts.values, color='lightgreen', alpha=0.7)\n",
    "    ax4.set_xlabel(feature, fontweight='bold')\n",
    "    ax4.set_ylabel('Sample Size', fontweight='bold')\n",
    "    ax4.set_title(f'Sample Size by {feature}', fontweight='bold')\n",
    "    ax4.set_xticks(range(len(counts)))\n",
    "    ax4.set_xticklabels(counts.index, rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value) in enumerate(zip(ax4.patches, counts.values)):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,\n",
    "                f'{int(value)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\n{feature} Statistics (sorted by mean LogSalePrice):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Category':<15} {'Count':<8} {'Mean':<8} {'Median':<8} {'Std':<8}\")\n",
    "    print(\"=\" * 70)\n",
    "    for category in category_order:\n",
    "        row = stats_by_category.loc[category]\n",
    "        print(f\"{str(category):<15} {int(row['count']):<8} {row['mean']:<8.3f} {row['median']:<8.3f} {row['std']:<8.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_relationship(df, \"KitchenQual\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
